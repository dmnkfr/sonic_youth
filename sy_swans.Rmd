---
title: 'Repetition and noise: The words of Sonic Youth and Swans'
author: Dominik Freunberger
output: 
  hrbrthemes::ipsum:
    code_folding: hide
    toc: true
editor_options:
  chunk_output_type: inline
  markdown:
    wrap: 80
---

```{r setup, include=FALSE, eval=TRUE}
library(tidyverse)
library(tidytext)
library(tidymodels)
library(caret)
library(quanteda)
library(quanteda.textstats)
library(geniusr)
library(tictoc)
library(knitr)
library(wordcloud)
library(hrbrthemes)
library(stm)
library(furrr)
library(topicmodels)
library(ldatuning)
library(topicdoc)
library(gridExtra)
library(widyr)
library(ggraph)
library(igraph)
library(irlba)
library(doParallel)
library(lsa)
library(randomForest)
library(keras)
library(ggforce)
library(shiny)
library(reticulate)


# Genius token: frBVSmYqTcAm9awzKE8HeNrsXFTSv_5cjdrFBTUh3oUnughftDiSBXf-V0UB5OZ2

# get the stopwords from the SMART source in tidytext stopwords
smart_stopwords = get_stopwords(source = "smart")
smart_stopwords = c(smart_stopwords$word, "we'r")

# define some color pals
sw_colors = c("#AE9166", "#AE2223", "#FFB50D", "#535250")
sy_colors = c("#832B27", "#CD5A42", "#B7C3DF", "#63666B")
other_colors = rep(c("#453E46", "#CF3E60", "#E68A5D", "#C9C79A", "#57989D"),3)

# Some helpers
sw_albums_year = data.frame(album = c("Filth", "Cop", "Greed", "Holy Money", "Children of God",
                      "The Burning World", "White Light from the Mouth of Infinity",
                      "Love of Life", "The Great Annihilator", "Soundtracks for the Blind",
                      "My Father Will Guide Me Up a Rope to the Sky", "The Seer",
                      "To Be Kind", "The Glowing Man", "leaving meaning."),
                      year = c(1983, 1984, 1986, 1986, 1987, 1989, 1991, 1992,
                      1995, 1996, 2010, 2012, 2014, 2016, 2019))

sw_albums_by_year = sw_albums_year$album

sy_albums_year = data.frame(album = c("Confusion Is Sex", "Bad Moon Rising", "EVOL",
                    "Sister", "Daydream Nation", "Goo", "Dirty",
                    "Experimental Jet Set, Trash and No Star",
                    "Washing Machine", "A Thousand Leaves", 
                    "NYC Ghosts & Flowers", "Murray Street", "Sonic Nurse",
                    "Rather Ripped", "The Eternal"),
                    year = c(1983, 1985, 1986, 1987, 1988, 1990, 1992, 1994,
                             1995, 1998, 2000, 2002, 2004, 2006, 2009))
                    
sy_albums_by_year = sy_albums_year$album

```
by Dominik Freunberger
<br/><br/>
[![](images/sy_swans.jpeg "Members of Sonic Youth and Swans")](https://www.reddit.com/r/OldSchoolCool/comments/m303cy/members_of_the_bands_that_would_become_sonic/)
<br/><br/>

|      ***No pain, no death, no fear, no hate***
|      ***No time, no now, no suffering***
|      ***No touch, no loss, no hand, no sense***
|      ***No wound, no waste, no lust, no fear***
|      ***No mind, no greed, no suffering***
|      ***No thought, no hurt, no hands to reach***
|      ***No knife, no words, no lie, no cure***
|      ***No need, no hate, no will, no speech***
|      (From the Swans song [*Screen Shot*)](https://www.youtube.com/watch?v=6qDq9eGUmMI)
<br/><br/>

|      ***Can you please pass me a jug of winter light?***
|      ***Fold me in an ocean's whim?***
|      ***In sweet corrosive fire light?***
|      ***In the city made of tin?***
|      ***Are you famous under the skin?***
|      ***Familiar with the things you wanted?***
|      ***Able now to take it all in?***
|      ***Making peace with every hole in the story?***
|      (From the Sonic Youth song [*NYC Ghosts & Flowers*)](https://www.youtube.com/watch?v=bqnkMEnU0iI)

<br/> <br/> 
The NYC no wave/noise rock/post punk bands **Sonic Youth** and **Swans** are not
only known for their eccentric and hypnotic noise landscapes but equally well for
their intricate lyrics. While Sonic Youth's lyrics are deeply rooted in the tradition-less tradition of
modern American poetry, listening to Swans often is reminiscent of going to a church full of noise when singer Michael Gira recites his ecclesiastical texts in their maelstrom of noise. What both bands share is their love for repetition and noise.

Here, I will take a deep dive into the words of each of the two bands' 15 studio
records between 1983 and 2019 and provide different perspectives on some features of their lyrical craft. We'll see some (uncanny) commonalities and defining differences. Some of the questions I try to answer here include:

- How many words do they use on each record and on each of their songs?
- Which words do they use often and which words appear in many songs?
- Which words occur together often?
- How repetitive are the two bands? Has this changed over the years?
- How negative are their lyrics? Has this changed over the years?
- Can we teach a computer to differentiate between the two bands based on only a single line of lyrics?
- Can a computer write lyrics in their style for us?

So, let's get started.

[![](images/swans.jpeg "Swans circa 1983 by Catherine Ceresole")](https://www.vice.com/sv/article/rjyng6/swans-filth-reissue-interview)

<br/> <br/> 

I'll get the lyrics from Genius' API. You need a genius account and
create an API client on <https://genius.com/api-clients> for this.

```{r get-lyrics, eval=FALSE, include=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# enter genius token when prompted:
genius_token()

# Find artist ID for Swans
artist = search_artist("Swans")
songs = get_artist_songs_df(artist$artist_id[1]) 

# Get all song IDs
ids = c(as.character(songs$song_id))

# Create empty dataframe for the lyrics
sw_lyrics = data.frame()

# Add lyrics to df. This takes a long while.
# for (id in ids) {
#   lyrics = rbind(get_lyrics_id(id), lyrics)
#   print(id)
# }

# The loop above crashes after a while for an unknown reason. 
# I found the solution below on
# https://www.r-bloggers.com/2021/01/scraping-analysing-and-visualising-lyrics-in-r/
tic()
while (length(ids) > 0) {
  for (id in ids) {
    tryCatch({
      sw_lyrics = rbind(get_lyrics_id(id), sw_lyrics)
      successful = unique(sw_lyrics$song_id)
      ids = ids[!ids %in% successful]
      print(paste("done - ", id))
      print(paste("New length is ", length(ids)))
    }, error = function(e){})
  }
}
toc()

# it get's only 244 of 265 songs listed on Genius; reason unknown. But okej.


# # We're missing 7 songs from the record "To be kind" and 5 from "The glowing man"
# # Let's try to get the manually
# glowing_man = c("Cloud of Forgetting", "Cloud of Unknowing", 
#   "The World Looks Red/The World Looks Black", "People Like Us",
#   "Frankie M.", "When Will I Return? (Ft. Jennifer Gira)", "The Glowing Man",
#   "Finally, Peace")
# 
# 
# kind = c("Screen Shot", "Just a Little Boy (For Chester Burnett)", 
#          "A Little God in My Hands", "Bring the Sun / Toussaint Lâ€™Ouverture",
#          "Some Things We Do", "She Loves Us", "Kirsten Supine", "Oxygen", 
#          "Nathalie Neal", "To Be Kind")
# 
# missing_ids = songs %>% 
#   filter(song_name %in% glowing_man|
#          song_name %in% kind |
#          grepl("When Will I", song_name)) %>% 
#   select(song_id, song_name)
# 
# # Create empty dataframe for the missing lyrics
# missing_lyrics = data.frame()
# 
# # Add lyrics to df. This takes about 4 secs per song
# for (id in missing_ids$song_id) {
#    missing_lyrics = rbind(get_lyrics_id(id), missing_lyrics)
#    print(id)
#  }
# 
# while (length(missing_ids$song_id) > 0) {
#   for (id in missing_ids$song_id) {
#     tryCatch({
#       missing_lyrics = rbind(get_lyrics_id(id), missing_lyrics)
#       successful = unique(missing_lyrics$song_id)
#       ids = ids[!ids %in% successful]
#       print(id)
#       print(paste("New length is ", length(ids)))
#     }, error = function(e){})
#   }
# }
# 
# # add missing lyrics to existing lyrics
# sw_lyrics = rbind(sw_lyrics, missing_lyrics)

# add album info
all_ids = data.frame(song_id = unique(sw_lyricss$song_id))
all_ids$album = ""

for (song in all_ids$song_id) {
  all_ids[match(song,all_ids$song_id),2] = get_song_df(song)[12]
  print(song)
}

# join lyrics with album info
sw_lyrics = full_join(all_ids, sw_lyrics)

#### No idea what's going on, but for two records some songs seem to be lost along the way.

# get only studio albums, get rid of unused cols and rename them
sw_lyrics = sw_lyrics %>%
  filter(album %in% sw_albums_year$album) %>% 
  left_join(sw_albums_year) %>% 
  select(artist_name, album, year, song_name, song_id, section_name, line) %>% 
  rename(artist = artist_name, song = song_name, section = section_name)

saveRDS(sw_lyrics, "./interim/swans_lyrics_tidy.RDS")

### Now the same for Sonic Youth
# Find artist ID for Sonic Youth
artist = search_artist("Sonic Youth")
songs = get_artist_songs_df(artist$artist_id[1]) 

# Get all song IDs
ids = c(as.character(songs$song_id))

# Create empty dataframe for the lyrics
sy_lyrics = data.frame()

# Add lyrics to df. This takes a long while.
# for (id in ids) {
#   lyrics = rbind(get_lyrics_id(id), lyrics)
#   print(id)
# }

# The loop above crashes after a while for an unknown reason. 
# I found the solution below on
# https://www.r-bloggers.com/2021/01/scraping-analysing-and-visualising-lyrics-in-r/
tic()
while (length(ids) > 0) {
  for (id in ids) {
    tryCatch({
      sy_lyrics = rbind(get_lyrics_id(id), sy_lyrics)
      successful = unique(sy_lyrics$song_id)
      ids = ids[!ids %in% successful]
      print(paste("done - ", id))
      print(paste("New length is ", length(ids)))
    }, error = function(e){})
  }
}
toc()

# it get's only 277 of 419 songs listed on Genius; reason unknown. But okej.
# add album info
all_ids = data.frame(song_id = unique(sy_lyrics$song_id))
all_ids$album = ""

for (song in all_ids$song_id) {
  all_ids[match(song,all_ids$song_id),2] = get_song_df(song)[12]
  print(song)
}

# join lyrics with album info
sy_lyrics = full_join(all_ids, sy_lyrics)

# get only studio albums, get rid of unused cols and rename them
sy_lyrics = sy_lyrics %>%
  filter(album %in% sy_albums_year$album) %>% 
  left_join(sy_albums_year) %>% 
  select(artist_name, album, year, song_name, song_id, section_name, line) %>% 
  rename(artist = artist_name, song = song_name, section = section_name) 

saveRDS(sy_lyrics, "./interim/sonic_youth_lyrics_tidy.RDS")

length(unique(sw_lyrics$song))
length(unique(sy_lyrics$song))

```
<br/> <br/> 

So, we got 4602 lines of poetry from 149 Sonic Youth song and 4407 lines of prayers from 146 Swans songs. Let's see how many songs there are on each of the records.

<br/> <br/> 

```{r combine-lyrics, eval=FALSE, include=FALSE}
sy_lyrics = readRDS("./interim/sonic_youth_lyrics_tidy.RDS")
sw_lyrics = readRDS("./interim/swans_lyrics_tidy.RDS")

### Combine the two and delete songs that are just the ablum art
lyrics = rbind(sy_lyrics, sw_lyrics)
lyrics = lyrics %>% 
  filter(!str_detect(song, "Album Art"))

saveRDS(lyrics, "./interim/lyrics_tidy.RDS")
````

```{r songs-per-record, echo=FALSE, fig.dim = c(14,5), message=FALSE}
lyrics = readRDS("./interim/lyrics_tidy.RDS")

sw_nr_songs_plot = lyrics %>% 
  filter(artist == "Swans") %>% 
  group_by(album, year) %>% 
  summarize(nr_of_songs = n_distinct(song)) %>% 
  ggplot(aes(x = reorder(album, -year), y = nr_of_songs)) + 
  geom_col(fill = sw_colors[3])+
  coord_flip()+
  theme_ipsum() +
  labs(x = "Album by year",
       y = " ",
       title = "Songs per album",
       subtitle = "Swans")

sy_nr_songs_plot = lyrics %>% 
  filter(artist == "Sonic Youth") %>% 
  group_by(album, year) %>% 
  summarize(nr_of_songs = n_distinct(song)) %>% 
  ggplot(aes(x = reorder(album, -year), y = nr_of_songs)) + 
  geom_col(fill = sy_colors[3])+
  coord_flip()+
  theme_ipsum() +
  labs(x = " ",
       y = "Number of songs",
       title = " ",
       subtitle = "Sonic Youth")

grid.arrange(sw_nr_songs_plot, sy_nr_songs_plot, ncol = 2)

```

<br/> <br/> 
Arrrrghrghrgrhgr! For reasons still unknown, we got lyrics for only 3/10 songs from "To Be Kind" and only 3/8 from "The Glowing Man". But okej, we'll take what we got and start spiraling in.

### How many words do they use on each album?

```{r plot-words-per-records, echo=FALSE, fig.dim = c(14,5), message=FALSE}
lyrics = readRDS("./interim/lyrics_tidy.RDS")

sw_nr_words_plot = lyrics %>% 
  filter(artist == "Swans") %>% 
  mutate(words_per_line = sapply(strsplit(line, " "), length)) %>% 
  group_by(year, album) %>% 
  summarize(number_of_words = sum(words_per_line)) %>% 
  ggplot(aes(x = reorder(album, -year), y = number_of_words)) + 
  geom_col(fill = sw_colors[3])+
  coord_flip()+
  theme_ipsum() +
  labs(x = "Album by year",
       y = " ",
       title = "Words per album",
       subtitle = "Swans")

sy_nr_words_plot = lyrics %>% 
  filter(artist == "Sonic Youth") %>% 
  mutate(words_per_line = sapply(strsplit(line, " "), length)) %>% 
  group_by(year, album) %>% 
  summarize(number_of_words = sum(words_per_line)) %>% 
  ggplot(aes(x = reorder(album, -year), y = number_of_words)) + 
  geom_col(fill = sy_colors[3])+
  coord_flip()+
  theme_ipsum() +
  labs(x = " ",
       y = "Number of words",
       title = " ",
       subtitle = "Sonic Youth")

grid.arrange(sw_nr_words_plot, sy_nr_words_plot, ncol = 2)
```


### How many words are there in each song?

A quick and dirty count of unique words per song. We exclude all the la la la's and uh uh uh's.

```{r plot-words-per-song, echo=FALSE, fig.dim = c(13,5), message=FALSE}
sw_words_per_song = lyrics %>% 
  filter(artist == "Swans") %>% 
  group_by(year, album, song) %>% 
  # paste all the lines per album
  summarize(album_lyrics = paste0(line, collapse = " ")) %>%
  # replace commas (they sometimes have no space afterwards, e.g. test,test)
  mutate(album_lyrics = gsub("\\,", " ", album_lyrics)) %>%
  # lowercase the words
  mutate(album_lyrics = tolower(album_lyrics)) %>%
  # delete special characters
  mutate(album_lyrics = str_replace_all(album_lyrics, "[[:punct:]]", " ")) %>% 
  mutate(unique_words_per_song = sapply(strsplit(album_lyrics, " "), unique)) %>% 
  mutate(number_unique_words_per_song = as.numeric(lapply(unique_words_per_song, length))) %>% 
  arrange(desc(number_unique_words_per_song))

# let's get the shortest and the longest
sw_short_long = sw_words_per_song %>% 
  filter(number_unique_words_per_song > 200 | number_unique_words_per_song < 3)

sw_words_per_song_plot = ggplot(data = sw_words_per_song, aes(x = reorder(album, -year), y = number_unique_words_per_song, label = song)) +
  geom_point(shape = 1, size = 3.3, color = sw_colors[3], alpha = 0.7, stroke = 1) +
  geom_point(data = sw_short_long, 
             aes(x = reorder(album, number_unique_words_per_song), 
                 y = number_unique_words_per_song),
             shape = 1, size = 3.3, color = sw_colors[2], stroke = 1) +
  geom_text(aes(label=ifelse(number_unique_words_per_song<3,as.character(song),'')),hjust=-0.1,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(number_unique_words_per_song>200,as.character(song),'')),hjust=1.05,vjust=0, size = 3) + 
  theme(legend.position = "none") +
  coord_flip()+
  theme_ipsum() +
  labs(x = "Album by year",
       y = " ",
       title = "Number of unique words",
       subtitle = "Swans")

### Sonic Youth

sy_words_per_song = lyrics %>% 
  filter(artist == "Sonic Youth") %>% 
  group_by(year, album, song) %>% 
  # paste all the lines per album
  summarize(album_lyrics = paste0(line, collapse = " ")) %>%
  # replace commas (they sometimes have no space afterwards, e.g. test,test)
  mutate(album_lyrics = gsub("\\,", " ", album_lyrics)) %>%
  # lowercase the words
  mutate(album_lyrics = tolower(album_lyrics)) %>%
  # delete special characters
  mutate(album_lyrics = str_replace_all(album_lyrics, "[[:punct:]]", " ")) %>% 
  mutate(unique_words_per_song = sapply(strsplit(album_lyrics, " "), unique)) %>% 
  mutate(number_unique_words_per_song = as.numeric(lapply(unique_words_per_song, length))) %>% 
  arrange(desc(number_unique_words_per_song))

# let's get the shortest and the longest
sy_short_long = sy_words_per_song %>% 
  filter(number_unique_words_per_song > 200 | number_unique_words_per_song < 6)

sy_words_per_song_plot = ggplot(data = sy_words_per_song, aes(x = reorder(album, -year), y = number_unique_words_per_song, label = song)) +
  geom_point(shape = 1, size = 3.3, color = sy_colors[3], alpha = 0.7, stroke = 1) +
  geom_point(data = sy_short_long, 
             aes(x = reorder(album, number_unique_words_per_song), 
                 y = number_unique_words_per_song),
             shape = 1, size = 3.3, color = sy_colors[2], stroke = 1) +
  geom_text(aes(label=ifelse(number_unique_words_per_song<6,as.character(song),'')),hjust=-0.1,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(number_unique_words_per_song>200,as.character(song),'')),hjust=1.05,vjust=0, size = 3) + 
  theme(legend.position = "none") +
  coord_flip()+
  theme_ipsum() +
  labs(x = " ",
       y = "Number of words",
       title = " ",
       subtitle = "Sonic Youth")

grid.arrange(sw_words_per_song_plot, sy_words_per_song_plot, ncol = 2)

```

|          ***AAAAH*** 
|          ***AAAAH*** 
|          ***AAAAH***
|          ***AAAAH***
|          ***AAAAH***
|          ***AAAAH***
|          ***AAAAH***
|          (From the Swans song [*YRP2*)](https://www.youtube.com/watch?v=JxK1WJR8yoo)
<br/>
Doesn't take (m)any words for a shaman to do their job. Brains i/o.

<br/><br/>

|          ***Lightning***
|          ***Lightning strike***
|          ***Lightning strikes***
|          ***Lightning strike me***
|          ***Lightning strikes me***
|          ***Lightning strike me down***
|          ***Lightning strikes me down***
|          (From the Sonic Youth song [*Lightning*)](https://www.youtube.com/watch?v=N_64Sh42uW4)
<br/>
Repetitive lyrics always get me! Five unique words (because we did
not stem *strikes* to *strike*, yet)! Though, admittedly, this closing
song is clearly an ode to anti-art and rather a convulsion of guitar
squeaks and spoken boredom. <br/> The other end of this spectrum marks
the song [*In the Kingdom
\#19*](https://www.youtube.com/watch?v=jUegc5J8iZk), a gripping [piece
of poetry](https://genius.com/Sonic-youth-in-the-kingdom-19-lyrics) with
dystopic guitars that seems to be a young descendant of
[*Howl*](https://www.poetryfoundation.org/poems/49303/howl). 230 unique
words, and, except for the chorus, quite the opposite of repetitiveness.

<br/>
Now that we have some ballpark figures about how short or long
their lyrics are, let's have a look at what words they actually use (a
lot, a lot).

### Which words do they use (often)?

```{r process-tokens, eval=FALSE, warning=FALSE, echo=FALSE}
# concatenate lines to lyrics per song (so that each song is a document )
sw_lyrics_song = sw_lyrics %>% 
  group_by(artist, album, year, song) %>% 
  summarise(lyrics = paste0(line, collapse = " "))

# tokenize
sw_tokens = tokens(sw_lyrics_song$lyrics, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
sw_tokens = tokens_wordstem(sw_tokens, language = "english")

# Lowercase the tokens
sw_tokens = tokens_tolower(sw_tokens)

# Remove stopwords
sw_tokens = tokens_remove(sw_tokens, c(stopwords("english"),
                                       smart_stopwords, "you'r"))
# remove two letter tokens
sw_tokens = tokens_select(sw_tokens, min_nchar = 3)

# add meaningful doc names instead of text 1, text 2, ...
doc_id = paste(1:nrow(sw_lyrics_song), sw_lyrics_song$song_id, sep = "_")
docnames(sw_tokens) <- doc_id

# Create bag-of-words model (document frequency matrix/ dfm)
sw_dfm = dfm(sw_tokens, tolower = FALSE)

# calculate frequencies
sw_word_freq = textstat_frequency(sw_dfm, n = 200)

### the same for sonic youth
sy_lyrics_song = sy_lyrics %>% 
     group_by(artist, album, year, song) %>% 
    summarise(lyrics = paste0(line, collapse = " "))

sy_tokens = tokens(sy_lyrics_song$lyrics, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
sy_tokens = tokens_wordstem(sy_tokens, language = "english")

# Lowercase the tokens
sy_tokens = tokens_tolower(sy_tokens)

# Remove stopwords
sy_tokens = tokens_remove(sy_tokens, 
                                      c(stopwords("english"), 
                                        smart_stopwords, "you'r"))
# remove two letter tokens
sy_tokens = tokens_select(sy_tokens, min_nchar = 3)

# add meaningful doc names instead of text 1, text 2, ...
doc_id = paste(1:nrow(sy_lyrics_song), sy_lyrics_song$song_id, sep = "_")
docnames(sy_tokens) <- doc_id

# Create bag-of-words model (document frequency matrix/ dfm)
sy_dfm = dfm(sy_tokens, tolower = FALSE)

# calculate frequencies
sy_word_freq = textstat_frequency(sy_dfm, n = 200)

saveRDS(sy_word_freq, "./interim/sy_word_freq.RDS")
saveRDS(sw_word_freq, "./interim/sw_word_freq.RDS")
```

Here is a __word cloud__ of the words they use most often. Note that some words look a bit strange ( _littl?!_ _someth??!_). That's because the words were [_stemmed_.](https://en.wikipedia.org/wiki/Stemming)

```{r wordclouds, echo=FALSE, fig.dim=c(13,8), warning=FALSE}
# word cloud with the n most frequent terms
sy_word_freq = readRDS("./interim/sy_word_freq.RDS")
sw_word_freq = readRDS("./interim/sw_word_freq.RDS")

par(mfrow=c(1,2))
set.seed(42)
wordcloud(words = sw_word_freq$feature, 
          freq = sw_word_freq$frequency, max.words = 100,
          random.order = FALSE,         
          color = sw_colors)

wordcloud(words = sy_word_freq$feature, 
          freq = sy_word_freq$frequency, max.words = 100,
          random.order = FALSE,         
          color = sy_colors)
```
<br/>
Time! Believe! Hey! And * drum rolls * __Love!__ Music that sounds like a sledgehammer at times is no excuse to avoid the big L. It's astonishing how different the two clouds look, pointing to their distinct vocabulary. Many of the words immediately trigger an [__Ohrwurm__](https://en.wiktionary.org/wiki/Ohrwurm).
<br/>
Now let's have a look at how often they use __frequent words they have in common__ and in how many songs they use them.

```{r frequent_words, message=FALSE, fig.dim=c(8,5), echo=FALSE, warning=FALSE}
# frequent words
sw_frequent_words = as.data.frame(sw_word_freq)
sy_frequent_words = as.data.frame(sy_word_freq)

frequent_words = rbind(sw_frequent_words, sy_frequent_words)
frequent_words$artist = rep(c("Swans", "Sonic Youth"), each = 200)
  
frequent_words %>%
  rename(word = feature) %>% 
  group_by(word) %>% 
  filter(frequency > 28) %>% 
    filter(n()>1) %>%
  group_by(artist) %>% 
  ggplot(aes(x = reorder(word, frequency), y = reorder(artist, desc(artist)))) +
  geom_point(aes(size = frequency), 
             shape = 1, color = rep(c(sw_colors[3], sy_colors[3]), each = 14), stroke = 1.2,
             position = position_nudge(y = -0.06)) +
  geom_point(aes(size = docfreq), 
             shape = 16, color = rep(c(sw_colors[3], sy_colors[3]), each = 14), stroke = 1.2,
             position = position_nudge(y = 0.06)) +
  geom_text(aes(label=ifelse(frequency > 200,paste(as.character(frequency), " times"),"")),
            hjust=1.5,vjust=0, size = 3) +
    geom_text(aes(label=ifelse(artist == "Swans" & word == "mind",paste("on ", as.character(docfreq), " songs"),"")),
            hjust=-0.4,vjust=0, size = 3) +
    geom_text(aes(label=ifelse(docfreq > 60,paste("on ", as.character(docfreq), " songs"),"")),
            hjust=-0.4,vjust=0, size = 3) +
      geom_text(aes(label=ifelse(artist == "Sonic Youth" & word == "time",paste(as.character(frequency), " times"),"")),
            hjust=1.5,vjust=0, size = 3) +
  coord_flip() +
  theme_ipsum() +
  labs(x = "Word",
       y = "Artist",
       title = "Most used words of both bands",
       subtitle = "Word frequency and number of songs the word occurs in",
       size = "Frequency")

```
<br/>
230 times __love!__ In a mere 36 Swans songs. Sonic Youth doesn't use it as often but in 49 songs. This trophy of repetitiveness goes to Swans! Sonic Youth, on the other hand, uses the word _eye_ in almost half of their songs.
<br/>
<br/>


|          ***And love will save you***
|          ***From the truth when you think you're free***
|          ***And love will save you***
|          ***From the cold light of boring reality***
|          ***And love will save you***
|          ***From the corruption of your lazy-minded soul***
|          ***And love will save you***
|          ***From your selfish and distorted goals***
|          (From the Swans song [*Love will save you*)](https://www.youtube.com/watch?v=HXbcErP-Xok)
<br/>
<br/>

### Mantras of noise: How repetitive are their lyrics?

```{r type-token, echo=FALSE, message=FALSE, warning=FALSE, fig.dim = c(9,5)}
# concatenate lines to lyrics per song (so that each song is a document)
lyrics_song = lyrics %>% 
  group_by(artist, album, year, song) %>% 
  summarise(lyrics = paste0(line, collapse = " "))

# Tokenize all lyrics
tokens_lyrics = tokens(lyrics_song$lyrics, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
tokens_lyrics = tokens_wordstem(tokens_lyrics, language = "english")

# Lowercase the tokens
tokens_lyrics = tokens_tolower(tokens_lyrics)

# Remove stopwords
tokens_lyrics = tokens_remove(tokens_lyrics, 
                                      c(stopwords("english"), 
                                        smart_stopwords, "you'r"))
# remove two letter tokens
tokens_lyrics = tokens_select(tokens_lyrics, min_nchar = 3)

# Add bigrams and trigrams to our feature matrix
tokens_lyrics = tokens_ngrams(tokens_lyrics, n = 1:3)


# get regulr type-toke-ratio
# the closer to 1, the greater the lexical variety
ttr = textstat_lexdiv(tokens_lyrics, measure = "TTR")
ttr = cbind(ttr, lyrics_song)

# plot
ttr %>% 
  group_by(artist, year, album) %>% 
  summarise(mean_ttr = mean(TTR)) %>% 
  ggplot(aes(x = year, y = mean_ttr)) +
  geom_line(aes(color = reorder(artist, desc(artist))), alpha = .5) + 
  geom_point(aes(x = year, y = mean_ttr), color = rep(c(sy_colors[3], sw_colors[3]), each = 15)) +
  geom_smooth(method = "loess", span = 0.5, aes(color = artist), se = FALSE, size = 2) +
  scale_color_manual(values=c(sw_colors[3], sy_colors[3])) +
  geom_text(aes(label=ifelse(artist == "Swans" & mean_ttr < .33, as.character(album), "")),
            hjust=-0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Swans" & mean_ttr > 0.86, "My Father Will Guide Me \n up a Rope to the Sky", "")),
            hjust=0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Sonic Youth" & mean_ttr < 0.5, as.character(album), "")),
            hjust=-0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Sonic Youth" & mean_ttr > 0.841, as.character(album), "")),
            hjust=0,vjust=0, size = 3) +
  theme_ipsum() +
  scale_y_reverse()+
  labs(x = "Year",
       y = "Mean Type-Token-Ratio per Album",
       title = "How repetitive are the lyrics?",
       subtitle = "Analysis of lexical variety of each album's lyrics",
       color = "Artist")

```

```{r word-cooccurence, echo=FALSE, warning=FALSE, message=FALSE, fig.dim = c(7,5)}

sy_coo = lyrics %>% 
  filter(artist == "Sonic Youth") %>% 
  ungroup() %>%  
  unnest_tokens(word, line) %>%
  distinct() %>%
  filter(!word %in% smart_stopwords) %>%
  anti_join(stop_words) %>%
  filter(nchar(word) > 2)

sy_word_corr = sy_coo %>% 
  group_by(word) %>%
  filter(n() >= 10) %>%
  pairwise_cor(word, section, sort = TRUE)

sy_coo_plot = sy_word_corr %>%
  filter(correlation > .70) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = sy_colors[3], size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

sw_coo = lyrics %>% 
  filter(artist == "Swans") %>% 
  ungroup() %>%  
  unnest_tokens(word, line) %>%
  distinct() %>%
  filter(!word %in% smart_stopwords) %>%
  anti_join(stop_words) %>%
  filter(nchar(word) > 2)

sw_word_corr = sw_coo %>% 
  group_by(word) %>%
  filter(n() >= 15) %>%
  pairwise_cor(word, section, sort = TRUE)

sw_coo_plot = sw_word_corr %>%
  filter(correlation > .75) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = sw_colors[3], size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

grid.arrange(sw_coo_plot, sy_coo_plot, ncol = 2)

```

```{r sentiment-analysis, message=FALSE, echo=FALSE, fig.dim=c(9,5), warning=FALSE}
# Tokenize all lyrics
tokens_lyrics_line = tokens(lyrics$line, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
tokens_lyrics_line = tokens_wordstem(tokens_lyrics_line, language = "english")

# Lowercase the tokens
tokens_lyrics_line = tokens_tolower(tokens_lyrics_line)

# Remove stopwords
tokens_lyrics_line = tokens_remove(tokens_lyrics_line, 
                                      c(stopwords("english"), 
                                        smart_stopwords, "you'r"))
# remove two letter tokens
tokens_lyrics_line = tokens_select(tokens_lyrics_line, min_nchar = 3)

# get only pos and neg values from dict
data_dictionary_LSD2015_pos_neg = data_dictionary_LSD2015[1:2]

# get sentiments
tokens_lsd = tokens_lookup(tokens_lyrics_line, dictionary = data_dictionary_LSD2015_pos_neg)

# convert to df
dfm_lsd = dfm(tokens_lsd)
lyrics_sentiment = convert(dfm_lsd, to = "data.frame")

# get info back
lyrics_sentiment = cbind(lyrics_sentiment, lyrics)

# plot
lyrics_sentiment %>% 
  group_by(artist, year, album) %>% 
  summarize(positive = mean(positive),
            negative = mean(negative),
            sentiment = positive-negative) %>% 
  ggplot(aes(x = year, y = sentiment)) +
  geom_line(aes(color = reorder(artist, desc(artist))), alpha = .5) + 
  geom_point(aes(x = year, y = sentiment), color = rep(c(sy_colors[3], sw_colors[3]), each = 15)) +
  geom_smooth(method = "loess", aes(color = artist), se = FALSE, size = 2) +
  scale_color_manual(values=c(sw_colors[3], sy_colors[3])) +
  geom_text(aes(label=ifelse(artist == "Swans" & sentiment < -0.5, as.character(album), "")),
            hjust=-0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Swans" & sentiment > 0.22, as.character(album), "")),
            hjust=0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Sonic Youth" & sentiment < -0.3, as.character(album), "")),
            hjust=-0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Sonic Youth" & sentiment > 0.115, as.character(album), "")),
            hjust=0,vjust=0, size = 3) +
  theme_ipsum() +
  labs(x = "Year",
       y = "Mean Sentiment per Album",
       title = "How negative are the lyrics?",
       subtitle = "Sentiment analysis of each album's lyrics",
       color = "Artist")

```

```{r ml-classification-6, eval=FALSE, include=FALSE}
# Let's try some tensor flow models

# get only text and label (=artist)
all_lyrics = readRDS("./interim/all_lyrics.RDS")

ml_lyrics = all_lyrics %>% 
  select(line, artist, song) %>% 
  group_by(artist, song) %>% 
  # paste all the lines per album
  summarize(lyrics = paste0(line, collapse = " ")) %>% 
  select(artist, lyrics)

# Use caret to create a 70%/30% stratified split. Set the random
# seed for reproducibility
set.seed(42)
indexes = createDataPartition(ml_lyrics$artist, times = 1,
                               p = 0.7, list = FALSE)

train = ml_lyrics[indexes,]
test = ml_lyrics[-indexes,]

# Verify proportions
prop.table(table(train$artist))
prop.table(table(test$artist))

# https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/

num_words = 10000
max_length = 50
text_vectorization = layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length,
)

text_vectorization %>% 
  adapt(ml_lyrics$line)

# TODO see https://github.com/tensorflow/tensorflow/pull/34529
get_vocabulary(text_vectorization)

text_vectorization(matrix(ml_lyrics$line[1], ncol = 1))

# build the model
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(input, output)

# Now, configure the model to use an optimizer and a loss function:
model %>% compile(
  optimizer = 'rmsprop',
  loss = 'binary_crossentropy',
  metrics = list('accuracy')
)

history = model %>% fit(
  train$line,
  as.numeric(train$artist == "Sonic Youth"),
  epochs = 500,
  batch_size = 512,
  validation_split = 0.2,
  verbose=2
)

results <- model %>% evaluate(test$line, as.numeric(test$artist == "Sonic Youth"), verbose = 0)
results
# .8020 accuracy on the test set. This is decent and will do!
plot(history)

# save model
model %>% save_model_tf("model")

```

```{r build-lyrics-classifier, fig.dim = c(7,4), echo=FALSE}
# load model
model <- load_model_tf("model")
# define input
input = "breakfast is great"

# make prediction
pred = model %>% predict(input)
predictions = data.frame(prediction = "prediction",
                         pred = c(0, pred-0.5))

color = ifelse(predictions$pred > 0, sy_colors[3], sw_colors[3])

ggplot(predictions, aes(x=prediction,y=pred[2]))+
            geom_point(size = abs(predictions$pred)*80, color = color)+
            ylim(-.7,.7)+
            geom_point(aes(x = 0.7, y = 0), color = "grey", size = 3)+
            geom_point(aes(x = 0.7, y = .1), color = "grey", size = 6)+
            geom_point(aes(x = 0.7, y = .24), color = "grey", size = 12)+
            geom_point(aes(x = 0.7, y = -.1), color = "grey", size = 6)+
            geom_point(aes(x = 0.7, y = -.24), color = "grey", size = 12)+
            geom_vline(xintercept = 1, color = "grey", alpha = .2)+
            geom_hline(yintercept = 0, color = "grey", alpha = .2)+
        
            annotate("text", x = 1.3, y = 0, label = "< sounds like >",
                     colour="grey", size=5, family="Courier")+          
  
            coord_flip()+
            annotate("text", x = 1, y = -0.5, label = "Swans",
                     colour="#535250", size=7, family="Courier")+
            annotate("text", x = 1, y = 0.5, label = "Sonic Youth",
                     colour="#535250", size=7, family="Courier")+
            theme_void()

```


[![](images/example_gif.gif "Lyrics Classification Example")](https://dmnkfr.shinyapps.io/Swans_Sonic_Youth/)
<br/><br/>

```{python build-lyrics-generator}
# Following this tutorial
# https://levelup.gitconnected.com/lyrics-generation-using-lstm-5a5a0bcac4fa
# get libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.layers import Embedding, Bidirectional, Dropout, Dense, LSTM
from keras.models import Sequential
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
tf.__version__

# load data
data = open('./interim/only_lyrics.txt').read()
# lowercase and split
corpus = data.lower().split("\n")
corpus = [x for x in corpus if not isinstance(x, int)]

# To make sure no sentence appears twice in our corpus, we use set. Otherwise, it will make the model biased.
corpus = list(set(corpus))

# tokenize it with Keras' Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1

# create input sequences using list of tokens
input_sequences = []
for line in corpus:
    token_list = tokenizer.texts_to_sequences([line])[0]
    
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
    
    input_sequences.append(n_gram_sequence)

# Pad Sequences: In our corpus, every sentence has a different length. So the input sequence will also have different lengths. Such an input cannot be fed to the model because it is unbalanced. Hence, we perform padding.

# pad sequences
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences,
                       maxlen = max_sequence_len, padding='pre'))


# Predictors and Labels: The problem we are solving here is a supervised learning problem. So we will have to provide the model with some labels so that it can generalise the relation between the words used to predict and the predicted word.
# So, we will use our input sequence and use the last word of all sequences as labels for all previous words.

predictors, label = input_sequences[:,:-1],input_sequences[:,-1]


# Building the Model
# Honestly, the complex part is done. Now we just have to build a RNN model using LSTM. Read more about LSTM here.
model = Sequential()
model.add(Embedding(total_words, 50, input_length=max_sequence_len-1))
# Add an LSTM Layer
model.add(Bidirectional(LSTM(150, return_sequences=True)))  
# A dropout layer for regularisation
model.add(Dropout(0.2))
# Add another LSTM Layer
model.add(LSTM(100)) 
model.add(Dense(total_words/2, activation='relu'))  
# In the last layer, the shape should be equal to the total number of words present in our corpus
model.add(Dense(total_words, activation='softmax'))
# change loss to "sparse_categorical_crossentropy"
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')  #(# Pick a loss function and an optimizer)
print(model.summary())

# Now, we will fit the model on predictors and labels. You can vary the epochs to see the variation in accuracy and where the model overfits the data.
history = model.fit(predictors, label, epochs= 100, verbose=1)


# Generating Lyrics
# We will generate lyrics now, using the model we built.
def make_lyrics(seed_text, next_words):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list],
                     maxlen=max_sequence_len-1,padding='pre')
        predict = model.predict(token_list)
        predicted_word = np.argmax(predict,axis=1)
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_word:
                output_word = word
                break
        seed_text += " " + output_word
    print(seed_text)
    
model.save("./lyrics_generator")

make_lyrics("The love", 7)


# TODO: Translate this function to R!
```