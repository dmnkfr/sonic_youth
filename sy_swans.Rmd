---
title: 'Repetition and noise: The words of Sonic Youth and Swans'
author: Dominik Freunberger
output: 
  hrbrthemes::ipsum:
    code_folding: hide
    toc: true
editor_options:
  chunk_output_type: inline
  markdown:
    wrap: 80
---

```{r setup, include=FALSE, eval=TRUE}
library(tidyverse)
library(tidytext)
library(tidymodels)
library(caret)
library(quanteda)
library(quanteda.textstats)
library(geniusr)
library(tictoc)
library(knitr)
library(wordcloud)
library(hrbrthemes)
library(stm)
library(furrr)
library(topicmodels)
library(ldatuning)
library(topicdoc)
library(gridExtra)
library(widyr)
library(ggraph)
library(igraph)
library(irlba)
library(doParallel)
library(lsa)
library(randomForest)
library(keras)
library(ggforce)
library(shiny)


# Genius token: frBVSmYqTcAm9awzKE8HeNrsXFTSv_5cjdrFBTUh3oUnughftDiSBXf-V0UB5OZ2

# get the stopwords from the SMART source in tidytext stopwords
smart_stopwords = get_stopwords(source = "smart")
smart_stopwords = c(smart_stopwords$word, "we'r")

# define some color pals
sw_colors = c("#AE9166", "#AE2223", "#FFB50D", "#535250")
sy_colors = c("#832B27", "#CD5A42", "#B7C3DF", "#63666B")
other_colors = rep(c("#453E46", "#CF3E60", "#E68A5D", "#C9C79A", "#57989D"),3)

# Some helpers
sw_albums_year = data.frame(album = c("Filth", "Cop", "Greed", "Holy Money", "Children of God",
                      "The Burning World", "White Light from the Mouth of Infinity",
                      "Love of Life", "The Great Annihilator", "Soundtracks for the Blind",
                      "My Father Will Guide Me Up a Rope to the Sky", "The Seer",
                      "To Be Kind", "The Glowing Man", "leaving meaning."),
                      year = c(1983, 1984, 1986, 1986, 1987, 1989, 1991, 1992,
                      1995, 1996, 2010, 2012, 2014, 2016, 2019))

sw_albums_by_year = sw_albums_year$album

sy_albums_year = data.frame(album = c("Confusion Is Sex", "Bad Moon Rising", "EVOL",
                    "Sister", "Daydream Nation", "Goo", "Dirty",
                    "Experimental Jet Set, Trash and No Star",
                    "Washing Machine", "A Thousand Leaves", 
                    "NYC Ghosts & Flowers", "Murray Street", "Sonic Nurse",
                    "Rather Ripped", "The Eternal"),
                    year = c(1983, 1985, 1986, 1987, 1988, 1990, 1992, 1994,
                             1995, 1998, 2000, 2002, 2004, 2006, 2009))
                    
sy_albums_by_year = sy_albums_year$album

```
by Dominik Freunberger
<br/><br/>
[![](images/sy_swans.jpeg "Members of Sonic Youth and Swans")](https://www.reddit.com/r/OldSchoolCool/comments/m303cy/members_of_the_bands_that_would_become_sonic/)
<br/><br/>

|      ***No pain, no death, no fear, no hate***
|      ***No time, no now, no suffering***
|      ***No touch, no loss, no hand, no sense***
|      ***No wound, no waste, no lust, no fear***
|      ***No mind, no greed, no suffering***
|      ***No thought, no hurt, no hands to reach***
|      ***No knife, no words, no lie, no cure***
|      ***No need, no hate, no will, no speech***
|      (From the Swans song [*Screen Shot*)](https://www.youtube.com/watch?v=6qDq9eGUmMI)
<br/><br/>

|      ***Can you please pass me a jug of winter light?***
|      ***Fold me in an ocean's whim?***
|      ***In sweet corrosive fire light?***
|      ***In the city made of tin?***
|      ***Are you famous under the skin?***
|      ***Familiar with the things you wanted?***
|      ***Able now to take it all in?***
|      ***Making peace with every hole in the story?***
|      (From the Sonic Youth song [*NYC Ghosts & Flowers*)](https://www.youtube.com/watch?v=bqnkMEnU0iI)

<br/> <br/> 
The NYC no wave/noise rock/post punk bands **Sonic Youth** and **Swans** are not
only known for their eccentric and hypnotic noise landscapes but equally well for
their intricate lyrics. While Sonic Youth's lyrics are deeply rooted in the tradition-less tradition of
modern American poetry, listening to Swans often is reminiscent of going to a church full of noise when singer Michael Gira recites his ecclesiastical texts in their maelstrom of noise. What both bands share is their love for repetition and noise.

Here, I will take a deep dive into the words of each of the two bands' 15 studio
records between 1983 and 2019 and provide different perspectives on some features of their lyrical craft. We'll see some (uncanny) commonalities and defining differences. Some of the questions I try to answer here include:

- How many words do they use on each record and on each of their songs?
- Which words do they use often and which words appear in many songs?
- Which words occur together often?
- How repetitive are the two bands? Has this changed over the years?
- How negative are their lyrics? Has this changed over the years?
- Can we teach a computer to differentiate between the two bands based on only a single line of lyrics?
- Can a computer write lyrics in their style for us?

So, let's get started.

[![](images/swans.jpeg "Swans circa 1983 by Catherine Ceresole")](https://www.vice.com/sv/article/rjyng6/swans-filth-reissue-interview)

<br/> <br/> 

I'll get the lyrics from Genius' API. You need a genius account and
create an API client on <https://genius.com/api-clients> for this.

```{r get-lyrics, eval=FALSE, include=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# enter genius token when prompted:
genius_token()

# Find artist ID for Swans
artist = search_artist("Swans")
songs = get_artist_songs_df(artist$artist_id[1]) 

# Get all song IDs
ids = c(as.character(songs$song_id))

# Create empty dataframe for the lyrics
sw_lyrics = data.frame()

# Add lyrics to df. This takes a long while.
# for (id in ids) {
#   lyrics = rbind(get_lyrics_id(id), lyrics)
#   print(id)
# }

# The loop above crashes after a while for an unknown reason. 
# I found the solution below on
# https://www.r-bloggers.com/2021/01/scraping-analysing-and-visualising-lyrics-in-r/
tic()
while (length(ids) > 0) {
  for (id in ids) {
    tryCatch({
      sw_lyrics = rbind(get_lyrics_id(id), sw_lyrics)
      successful = unique(sw_lyrics$song_id)
      ids = ids[!ids %in% successful]
      print(paste("done - ", id))
      print(paste("New length is ", length(ids)))
    }, error = function(e){})
  }
}
toc()

# it get's only 244 of 265 songs listed on Genius; reason unknown. But okej.


# # We're missing 7 songs from the record "To be kind" and 5 from "The glowing man"
# # Let's try to get the manually
# glowing_man = c("Cloud of Forgetting", "Cloud of Unknowing", 
#   "The World Looks Red/The World Looks Black", "People Like Us",
#   "Frankie M.", "When Will I Return? (Ft. Jennifer Gira)", "The Glowing Man",
#   "Finally, Peace")
# 
# 
# kind = c("Screen Shot", "Just a Little Boy (For Chester Burnett)", 
#          "A Little God in My Hands", "Bring the Sun / Toussaint Lâ€™Ouverture",
#          "Some Things We Do", "She Loves Us", "Kirsten Supine", "Oxygen", 
#          "Nathalie Neal", "To Be Kind")
# 
# missing_ids = songs %>% 
#   filter(song_name %in% glowing_man|
#          song_name %in% kind |
#          grepl("When Will I", song_name)) %>% 
#   select(song_id, song_name)
# 
# # Create empty dataframe for the missing lyrics
# missing_lyrics = data.frame()
# 
# # Add lyrics to df. This takes about 4 secs per song
# for (id in missing_ids$song_id) {
#    missing_lyrics = rbind(get_lyrics_id(id), missing_lyrics)
#    print(id)
#  }
# 
# while (length(missing_ids$song_id) > 0) {
#   for (id in missing_ids$song_id) {
#     tryCatch({
#       missing_lyrics = rbind(get_lyrics_id(id), missing_lyrics)
#       successful = unique(missing_lyrics$song_id)
#       ids = ids[!ids %in% successful]
#       print(id)
#       print(paste("New length is ", length(ids)))
#     }, error = function(e){})
#   }
# }
# 
# # add missing lyrics to existing lyrics
# sw_lyrics = rbind(sw_lyrics, missing_lyrics)

# add album info
all_ids = data.frame(song_id = unique(sw_lyricss$song_id))
all_ids$album = ""

for (song in all_ids$song_id) {
  all_ids[match(song,all_ids$song_id),2] = get_song_df(song)[12]
  print(song)
}

# join lyrics with album info
sw_lyrics = full_join(all_ids, sw_lyrics)

#### No idea what's going on, but for two records some songs seem to be lost along the way.

# get only studio albums, get rid of unused cols and rename them
sw_lyrics = sw_lyrics %>%
  filter(album %in% sw_albums_year$album) %>% 
  left_join(sw_albums_year) %>% 
  select(artist_name, album, year, song_name, song_id, section_name, line) %>% 
  rename(artist = artist_name, song = song_name, section = section_name)

saveRDS(sw_lyrics, "./interim/swans_lyrics_tidy.RDS")

### Now the same for Sonic Youth
# Find artist ID for Sonic Youth
artist = search_artist("Sonic Youth")
songs = get_artist_songs_df(artist$artist_id[1]) 

# Get all song IDs
ids = c(as.character(songs$song_id))

# Create empty dataframe for the lyrics
sy_lyrics = data.frame()

# Add lyrics to df. This takes a long while.
# for (id in ids) {
#   lyrics = rbind(get_lyrics_id(id), lyrics)
#   print(id)
# }

# The loop above crashes after a while for an unknown reason. 
# I found the solution below on
# https://www.r-bloggers.com/2021/01/scraping-analysing-and-visualising-lyrics-in-r/
tic()
while (length(ids) > 0) {
  for (id in ids) {
    tryCatch({
      sy_lyrics = rbind(get_lyrics_id(id), sy_lyrics)
      successful = unique(sy_lyrics$song_id)
      ids = ids[!ids %in% successful]
      print(paste("done - ", id))
      print(paste("New length is ", length(ids)))
    }, error = function(e){})
  }
}
toc()

# it get's only 277 of 419 songs listed on Genius; reason unknown. But okej.
# add album info
all_ids = data.frame(song_id = unique(sy_lyrics$song_id))
all_ids$album = ""

for (song in all_ids$song_id) {
  all_ids[match(song,all_ids$song_id),2] = get_song_df(song)[12]
  print(song)
}

# join lyrics with album info
sy_lyrics = full_join(all_ids, sy_lyrics)

# get only studio albums, get rid of unused cols and rename them
sy_lyrics = sy_lyrics %>%
  filter(album %in% sy_albums_year$album) %>% 
  left_join(sy_albums_year) %>% 
  select(artist_name, album, year, song_name, song_id, section_name, line) %>% 
  rename(artist = artist_name, song = song_name, section = section_name) 

saveRDS(sy_lyrics, "./interim/sonic_youth_lyrics_tidy.RDS")

length(unique(sw_lyrics$song))
length(unique(sy_lyrics$song))

```
<br/> <br/> 

So, we got 4602 lines of poetry from 149 Sonic Youth song and 4407 lines of prayers from 146 Swans songs. Let's see how many songs there are on each of the records.

<br/> <br/> 

```{r combine-lyrics, eval=FALSE, include=FALSE}
sy_lyrics = readRDS("./interim/sonic_youth_lyrics_tidy.RDS")
sw_lyrics = readRDS("./interim/swans_lyrics_tidy.RDS")

### Combine the two and delete songs that are just the ablum art
lyrics = rbind(sy_lyrics, sw_lyrics)
lyrics = lyrics %>% 
  filter(!str_detect(song, "Album Art"))

saveRDS(lyrics, "./interim/lyrics_tidy.RDS")
````

```{r songs-per-record, echo=FALSE, fig.dim = c(14,5), message=FALSE}
lyrics = readRDS("./interim/lyrics_tidy.RDS")

sw_nr_songs_plot = lyrics %>% 
  filter(artist == "Swans") %>% 
  group_by(album, year) %>% 
  summarize(nr_of_songs = n_distinct(song)) %>% 
  ggplot(aes(x = reorder(album, -year), y = nr_of_songs)) + 
  geom_col(fill = sw_colors[3])+
  coord_flip()+
  theme_ipsum() +
  labs(x = "Album by year",
       y = " ",
       title = "Songs per album",
       subtitle = "Swans")

sy_nr_songs_plot = lyrics %>% 
  filter(artist == "Sonic Youth") %>% 
  group_by(album, year) %>% 
  summarize(nr_of_songs = n_distinct(song)) %>% 
  ggplot(aes(x = reorder(album, -year), y = nr_of_songs)) + 
  geom_col(fill = sy_colors[3])+
  coord_flip()+
  theme_ipsum() +
  labs(x = " ",
       y = "Number of songs",
       title = " ",
       subtitle = "Sonic Youth")

grid.arrange(sw_nr_songs_plot, sy_nr_songs_plot, ncol = 2)

```

<br/> <br/> 
Arrrrghrghrgrhgr! For reasons still unknown, we got lyrics for only 3/10 songs from "To Be Kind" and only 3/8 from "The Glowing Man". But okej, we'll take what we got and start spiraling in.

### How many words do they use on each album?

```{r plot-words-per-records, echo=FALSE, fig.dim = c(14,5), message=FALSE}
lyrics = readRDS("./interim/lyrics_tidy.RDS")

sw_nr_words_plot = lyrics %>% 
  filter(artist == "Swans") %>% 
  mutate(words_per_line = sapply(strsplit(line, " "), length)) %>% 
  group_by(year, album) %>% 
  summarize(number_of_words = sum(words_per_line)) %>% 
  ggplot(aes(x = reorder(album, -year), y = number_of_words)) + 
  geom_col(fill = sw_colors[3])+
  coord_flip()+
  theme_ipsum() +
  labs(x = "Album by year",
       y = " ",
       title = "Words per album",
       subtitle = "Swans")

sy_nr_words_plot = lyrics %>% 
  filter(artist == "Sonic Youth") %>% 
  mutate(words_per_line = sapply(strsplit(line, " "), length)) %>% 
  group_by(year, album) %>% 
  summarize(number_of_words = sum(words_per_line)) %>% 
  ggplot(aes(x = reorder(album, -year), y = number_of_words)) + 
  geom_col(fill = sy_colors[3])+
  coord_flip()+
  theme_ipsum() +
  labs(x = " ",
       y = "Number of words",
       title = " ",
       subtitle = "Sonic Youth")

grid.arrange(sw_nr_words_plot, sy_nr_words_plot, ncol = 2)
```


### How many words are there in each song?

A quick and dirty count of unique words per song. We exclude all the la la la's and uh uh uh's.

```{r plot-words-per-song, echo=FALSE, fig.dim = c(13,5), message=FALSE}
sw_words_per_song = lyrics %>% 
  filter(artist == "Swans") %>% 
  group_by(year, album, song) %>% 
  # paste all the lines per album
  summarize(album_lyrics = paste0(line, collapse = " ")) %>%
  # replace commas (they sometimes have no space afterwards, e.g. test,test)
  mutate(album_lyrics = gsub("\\,", " ", album_lyrics)) %>%
  # lowercase the words
  mutate(album_lyrics = tolower(album_lyrics)) %>%
  # delete special characters
  mutate(album_lyrics = str_replace_all(album_lyrics, "[[:punct:]]", " ")) %>% 
  mutate(unique_words_per_song = sapply(strsplit(album_lyrics, " "), unique)) %>% 
  mutate(number_unique_words_per_song = as.numeric(lapply(unique_words_per_song, length))) %>% 
  arrange(desc(number_unique_words_per_song))

# let's get the shortest and the longest
sw_short_long = sw_words_per_song %>% 
  filter(number_unique_words_per_song > 200 | number_unique_words_per_song < 3)

sw_words_per_song_plot = ggplot(data = sw_words_per_song, aes(x = reorder(album, -year), y = number_unique_words_per_song, label = song)) +
  geom_point(shape = 1, size = 3.3, color = sw_colors[3], alpha = 0.7, stroke = 1) +
  geom_point(data = sw_short_long, 
             aes(x = reorder(album, number_unique_words_per_song), 
                 y = number_unique_words_per_song),
             shape = 1, size = 3.3, color = sw_colors[2], stroke = 1) +
  geom_text(aes(label=ifelse(number_unique_words_per_song<3,as.character(song),'')),hjust=-0.1,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(number_unique_words_per_song>200,as.character(song),'')),hjust=1.05,vjust=0, size = 3) + 
  theme(legend.position = "none") +
  coord_flip()+
  theme_ipsum() +
  labs(x = "Album by year",
       y = " ",
       title = "Number of unique words",
       subtitle = "Swans")

### Sonic Youth

sy_words_per_song = lyrics %>% 
  filter(artist == "Sonic Youth") %>% 
  group_by(year, album, song) %>% 
  # paste all the lines per album
  summarize(album_lyrics = paste0(line, collapse = " ")) %>%
  # replace commas (they sometimes have no space afterwards, e.g. test,test)
  mutate(album_lyrics = gsub("\\,", " ", album_lyrics)) %>%
  # lowercase the words
  mutate(album_lyrics = tolower(album_lyrics)) %>%
  # delete special characters
  mutate(album_lyrics = str_replace_all(album_lyrics, "[[:punct:]]", " ")) %>% 
  mutate(unique_words_per_song = sapply(strsplit(album_lyrics, " "), unique)) %>% 
  mutate(number_unique_words_per_song = as.numeric(lapply(unique_words_per_song, length))) %>% 
  arrange(desc(number_unique_words_per_song))

# let's get the shortest and the longest
sy_short_long = sy_words_per_song %>% 
  filter(number_unique_words_per_song > 200 | number_unique_words_per_song < 6)

sy_words_per_song_plot = ggplot(data = sy_words_per_song, aes(x = reorder(album, -year), y = number_unique_words_per_song, label = song)) +
  geom_point(shape = 1, size = 3.3, color = sy_colors[3], alpha = 0.7, stroke = 1) +
  geom_point(data = sy_short_long, 
             aes(x = reorder(album, number_unique_words_per_song), 
                 y = number_unique_words_per_song),
             shape = 1, size = 3.3, color = sy_colors[2], stroke = 1) +
  geom_text(aes(label=ifelse(number_unique_words_per_song<6,as.character(song),'')),hjust=-0.1,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(number_unique_words_per_song>200,as.character(song),'')),hjust=1.05,vjust=0, size = 3) + 
  theme(legend.position = "none") +
  coord_flip()+
  theme_ipsum() +
  labs(x = " ",
       y = "Number of words",
       title = " ",
       subtitle = "Sonic Youth")

grid.arrange(sw_words_per_song_plot, sy_words_per_song_plot, ncol = 2)

```

|          ***AAAAH*** 
|          ***AAAAH*** 
|          ***AAAAH***
|          ***AAAAH***
|          ***AAAAH***
|          ***AAAAH***
|          ***AAAAH***
|          (From the Swans song [*YRP2*)](https://www.youtube.com/watch?v=JxK1WJR8yoo)

<br/><br/>

|          ***Lightning***
|          ***Lightning strike***
|          ***Lightning strikes***
|          ***Lightning strike me***
|          ***Lightning strikes me***
|          ***Lightning strike me down***
|          ***Lightning strikes me down***
|          (From the Sonic Youth song [*Lightning*)](https://www.youtube.com/watch?v=N_64Sh42uW4)

<br/> Repetitive lyrics always get me! Five unique words (because we did
not stem *strikes* to *strike*, yet)! Though, admittedly, this closing
song is clearly an ode to anti-art and rather a convulsion of guitar
squeaks and spoken boredom. <br/> The other end of this spectrum marks
the song [*In the Kingdom
\#19*](https://www.youtube.com/watch?v=jUegc5J8iZk), a gripping [piece
of poetry](https://genius.com/Sonic-youth-in-the-kingdom-19-lyrics) with
dystopic guitars that seems to be a young descendant of
[*Howl*](https://www.poetryfoundation.org/poems/49303/howl). 230 unique
words, and, except for the chorus, quite the opposite of repetitiveness.
<br/> Now that we have some ballpark figures about how short or long
their lyrics are, let's have a look at what words they actually use (a
lot).

```{r process-tokens, eval=FALSE, warning=FALSE, echo=FALSE}
# concatenate lines to lyrics per song (so that each song is a document )
sw_lyrics_song = sw_lyrics %>% 
  group_by(artist, album, year, song) %>% 
  summarise(lyrics = paste0(line, collapse = " "))

# tokenize
sw_tokens = tokens(sw_lyrics_song$lyrics, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
sw_tokens = tokens_wordstem(sw_tokens, language = "english")

# Lowercase the tokens
sw_tokens = tokens_tolower(sw_tokens)

# Remove stopwords
sw_tokens = tokens_remove(sw_tokens, c(stopwords("english"),
                                       smart_stopwords, "you'r"))
# remove two letter tokens
sw_tokens = tokens_select(sw_tokens, min_nchar = 3)

# add meaningful doc names instead of text 1, text 2, ...
doc_id = paste(1:nrow(sw_lyrics_song), sw_lyrics_song$song_id, sep = "_")
docnames(sw_tokens) <- doc_id

# Create bag-of-words model (document frequency matrix/ dfm)
sw_dfm = dfm(sw_tokens, tolower = FALSE)

# calculate frequencies
sw_word_freq = textstat_frequency(sw_dfm, n = 200)

### the same for sonic youth
sy_lyrics_song = sy_lyrics %>% 
     group_by(artist, album, year, song) %>% 
    summarise(lyrics = paste0(line, collapse = " "))

sy_tokens = tokens(sy_lyrics_song$lyrics, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
sy_tokens = tokens_wordstem(sy_tokens, language = "english")

# Lowercase the tokens
sy_tokens = tokens_tolower(sy_tokens)

# Remove stopwords
sy_tokens = tokens_remove(sy_tokens, 
                                      c(stopwords("english"), 
                                        smart_stopwords, "you'r"))
# remove two letter tokens
sy_tokens = tokens_select(sy_tokens, min_nchar = 3)

# add meaningful doc names instead of text 1, text 2, ...
doc_id = paste(1:nrow(sy_lyrics_song), sy_lyrics_song$song_id, sep = "_")
docnames(sy_tokens) <- doc_id

# Create bag-of-words model (document frequency matrix/ dfm)
sy_dfm = dfm(sy_tokens, tolower = FALSE)

# calculate frequencies
sy_word_freq = textstat_frequency(sy_dfm, n = 200)

saveRDS(sy_word_freq, "./interim/sy_word_freq.RDS")
saveRDS(sw_word_freq, "./interim/sw_word_freq.RDS")
```

```{r wordclouds, echo=FALSE, fig.dim=c(13,8), warning=FALSE}
# word cloud with the n most frequent terms
sy_word_freq = readRDS("./interim/sy_word_freq.RDS")
sw_word_freq = readRDS("./interim/sw_word_freq.RDS")

par(mfrow=c(1,2))
set.seed(42)
wordcloud(words = sw_word_freq$feature, 
          freq = sw_word_freq$frequency, max.words = 100,
          random.order = FALSE,         
          color = sw_colors)

wordcloud(words = sy_word_freq$feature, 
          freq = sy_word_freq$frequency, max.words = 100,
          random.order = FALSE,         
          color = sy_colors)
```

```{r frequent_words, message=FALSE, fig.dim=c(7,5), echo=FALSE, warning=FALSE}
# frequent words
sw_frequent_words = as.data.frame(sw_word_freq)
sy_frequent_words = as.data.frame(sy_word_freq)

frequent_words = rbind(sw_frequent_words, sy_frequent_words)
frequent_words$artist = rep(c("Swans", "Sonic Youth"), each = 200)
  
frequent_words %>%
  rename(word = feature) %>% 
  group_by(word) %>% 
  filter(frequency > 28) %>% 
    filter(n()>1) %>%
  group_by(artist) %>% 
  ggplot(aes(x = reorder(word, frequency), y = reorder(artist, desc(artist)))) +
  geom_point(aes(size = frequency), 
             shape = 1, color = rep(c(sw_colors[3], sy_colors[3]), each = 14), stroke = 1.2,
             position = position_nudge(y = -0.06)) +
  geom_point(aes(size = docfreq), 
             shape = 16, color = rep(c(sw_colors[3], sy_colors[3]), each = 14), stroke = 1.2,
             position = position_nudge(y = 0.06)) +
  geom_text(aes(label=ifelse(frequency > 200,paste(as.character(frequency), " times"),"")),
            hjust=1.5,vjust=0, size = 3) +
    geom_text(aes(label=ifelse(artist == "Swans" & word == "mind",paste("on ", as.character(docfreq), " songs"),"")),
            hjust=-0.4,vjust=0, size = 3) +
    geom_text(aes(label=ifelse(docfreq > 60,paste("on ", as.character(docfreq), " songs"),"")),
            hjust=-0.4,vjust=0, size = 3) +
      geom_text(aes(label=ifelse(artist == "Sonic Youth" & word == "time",paste(as.character(frequency), " times"),"")),
            hjust=1.5,vjust=0, size = 3) +
  coord_flip() +
  theme_ipsum() +
  labs(x = "Word",
       y = "Artist",
       title = "Most used words of both bands",
       subtitle = "Word frequency and number of songs the word occurs in",
       size = "Frequency")

```

```{r type-token, echo=FALSE, message=FALSE, warning=FALSE}
# concatenate lines to lyrics per song (so that each song is a document)
lyrics_song = lyrics %>% 
  group_by(artist, album, year, song) %>% 
  summarise(lyrics = paste0(line, collapse = " "))

# Tokenize all lyrics
tokens_lyrics = tokens(lyrics_song$lyrics, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
tokens_lyrics = tokens_wordstem(tokens_lyrics, language = "english")

# Lowercase the tokens
tokens_lyrics = tokens_tolower(tokens_lyrics)

# Remove stopwords
tokens_lyrics = tokens_remove(tokens_lyrics, 
                                      c(stopwords("english"), 
                                        smart_stopwords, "you'r"))
# remove two letter tokens
tokens_lyrics = tokens_select(tokens_lyrics, min_nchar = 3)

# Add bigrams and trigrams to our feature matrix
tokens_lyrics = tokens_ngrams(tokens_lyrics, n = 1:3)


# get regulr type-toke-ratio
# the closer to 1, the greater the lexical variety
ttr = textstat_lexdiv(tokens_lyrics, measure = "TTR")
ttr = cbind(ttr, lyrics_song)

# plot
ttr %>% 
  group_by(artist, year, album) %>% 
  summarise(mean_ttr = mean(TTR)) %>% 
  ggplot(aes(x = year, y = mean_ttr)) +
  geom_line(aes(color = reorder(artist, desc(artist))), alpha = .5) + 
  geom_point(aes(x = year, y = mean_ttr), color = rep(c(sy_colors[3], sw_colors[3]), each = 15)) +
  geom_smooth(method = "loess", span = 0.5, aes(color = artist), se = FALSE, size = 2) +
  scale_color_manual(values=c(sw_colors[3], sy_colors[3])) +
  geom_text(aes(label=ifelse(artist == "Swans" & mean_ttr < .33, as.character(album), "")),
            hjust=-0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Swans" & mean_ttr > 0.86, as.character(album), "")),
            hjust=0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Sonic Youth" & mean_ttr < 0.5, as.character(album), "")),
            hjust=-0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Sonic Youth" & mean_ttr > 0.841, as.character(album), "")),
            hjust=0,vjust=0, size = 3) +
  theme_ipsum() +
  scale_y_reverse()+
  labs(x = "Year",
       y = "Mean Type-Token-Ratio per Album",
       title = "How repetitive are the lyrics?",
       subtitle = "Analysis of lexical variety of each album's lyrics",
       color = "Artist")

```

```{r word-cooccurence, echo=FALSE, warning=FALSE, message=FALSE}

sy_coo = lyrics %>% 
  filter(artist == "Sonic Youth") %>% 
  ungroup() %>%  
  unnest_tokens(word, line) %>%
  distinct() %>%
  filter(!word %in% smart_stopwords) %>%
  anti_join(stop_words) %>%
  filter(nchar(word) > 2)

sy_word_corr = sy_coo %>% 
  group_by(word) %>%
  filter(n() >= 10) %>%
  pairwise_cor(word, section, sort = TRUE)

sy_coo_plot = sy_word_corr %>%
  filter(correlation > .70) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = sy_colors[3], size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

sw_coo = lyrics %>% 
  filter(artist == "Swans") %>% 
  ungroup() %>%  
  unnest_tokens(word, line) %>%
  distinct() %>%
  filter(!word %in% smart_stopwords) %>%
  anti_join(stop_words) %>%
  filter(nchar(word) > 2)

sw_word_corr = sw_coo %>% 
  group_by(word) %>%
  filter(n() >= 15) %>%
  pairwise_cor(word, section, sort = TRUE)

sw_coo_plot = sw_word_corr %>%
  filter(correlation > .75) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = sw_colors[3], size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

grid.arrange(sw_coo_plot, sy_coo_plot, ncol = 2)

```

```{r sentiment-analysis, message=FALSE, echo=FALSE, fig.dim=c(7,5), warning=FALSE}
# Tokenize all lyrics
tokens_lyrics_line = tokens(lyrics$line, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
tokens_lyrics_line = tokens_wordstem(tokens_lyrics_line, language = "english")

# Lowercase the tokens
tokens_lyrics_line = tokens_tolower(tokens_lyrics_line)

# Remove stopwords
tokens_lyrics_line = tokens_remove(tokens_lyrics_line, 
                                      c(stopwords("english"), 
                                        smart_stopwords, "you'r"))
# remove two letter tokens
tokens_lyrics_line = tokens_select(tokens_lyrics_line, min_nchar = 3)

# get only pos and neg values from dict
data_dictionary_LSD2015_pos_neg = data_dictionary_LSD2015[1:2]

# get sentiments
tokens_lsd = tokens_lookup(tokens_lyrics_line, dictionary = data_dictionary_LSD2015_pos_neg)

# convert to df
dfm_lsd = dfm(tokens_lsd)
lyrics_sentiment = convert(dfm_lsd, to = "data.frame")

# get info back
lyrics_sentiment = cbind(lyrics_sentiment, lyrics)

# plot
lyrics_sentiment %>% 
  group_by(artist, year, album) %>% 
  summarize(positive = mean(positive),
            negative = mean(negative),
            sentiment = positive-negative) %>% 
  ggplot(aes(x = year, y = sentiment)) +
  geom_line(aes(color = reorder(artist, desc(artist))), alpha = .5) + 
  geom_point(aes(x = year, y = sentiment), color = rep(c(sy_colors[3], sw_colors[3]), each = 15)) +
  geom_smooth(method = "loess", aes(color = artist), se = FALSE, size = 2) +
  scale_color_manual(values=c(sw_colors[3], sy_colors[3])) +
  geom_text(aes(label=ifelse(artist == "Swans" & sentiment < -0.5, as.character(album), "")),
            hjust=-0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Swans" & sentiment > 0.22, as.character(album), "")),
            hjust=0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Sonic Youth" & sentiment < -0.3, as.character(album), "")),
            hjust=-0,vjust=0, size = 3) +
  geom_text(aes(label=ifelse(artist == "Sonic Youth" & sentiment > 0.115, as.character(album), "")),
            hjust=0,vjust=0, size = 3) +
  theme_ipsum() +
  labs(x = "Year",
       y = "Mean Sentiment per Album",
       title = "How negative are the lyrics?",
       subtitle = "Sentiment analysis of each album's lyrics",
       color = "Artist")

```

``` {r ml-classification, eval=FALSE}
# get only text and label (=artist)
lyrics = readRDS("./interim/lyrics_tidy.RDS")

ml_lyrics = lyrics %>% 
  select(line, artist)

# Use caret to create a 70%/30% stratified split. Set the random
# seed for reproducibility
set.seed(42)
indexes = createDataPartition(ml_lyrics$artist, times = 1,
                               p = 0.7, list = FALSE)

train = ml_lyrics[indexes,]
test = ml_lyrics[-indexes,]

# Verify proportions
prop.table(table(train$artist))
prop.table(table(test$artist))

# preprocess training text data
# tokenize
train_tokens = tokens(train$line, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Stem the tokens
train_tokens = tokens_wordstem(train_tokens, language = "english")

# Lowercase the tokens
train_tokens = tokens_tolower(train_tokens)

# Remove stopwords
train_tokens = tokens_remove(train_tokens, c(stopwords("english"),
                                       smart_stopwords, "you'r"))
# remove two letter tokens
train_tokens = tokens_select(train_tokens, min_nchar = 3)

# add bigrams 
train_tokens = tokens_ngrams(train_tokens, n = 1:3)

# Create bag-of-words model (document frequency matrix/ dfm)
train_dfm = dfm(train_tokens, tolower = FALSE)

# Transform to a matrix and inspect.
train_tokens_matrix = as.matrix(train_dfm)
#View(train_tokens_matrix[1:20, 1:100])
dim(train_tokens_matrix)

# Setup a the feature data frame with labels.
train_tokens_df = cbind(label = train$artist, data.frame(train_dfm))

# Cleanup column names.
names(train_tokens_df) = make.names(names(train_tokens_df))

# Use caret to create stratified folds for 10-fold cross validation repeated 
# 3 times (i.e., create 30 random stratified samples)
set.seed(42)
cv_folds = createMultiFolds(train$artist, k = 10, times = 3)

cv_cntrl <- trainControl(method = "repeatedcv", number = 10,
                         repeats = 3, index = cv_folds)


# Time the code execution
start.time <- Sys.time()

# train the model
rpart_cv1 <- train(label ~ ., data = train_tokens_df, method = "rpart", 
                    trControl = cv_cntrl, tuneLength = 7)

# Total time of execution on workstation was approximately 41 minutes. 
total.time <- Sys.time() - start.time
total.time

# Check out our results.
rpart_cv1

# .6012 accuracy is not too bad for a first attempt

#########################################
# Let's turn the dfm into a tf-idf-matrix

train_tokens_tfidf = dfm_tfidf(train_dfm)

# Make a clean data frame.
train_tokens_tfidf_df <- cbind(label = train$artist, data.frame(train_tokens_tfidf))
names(train_tokens_tfidf_df) <- make.names(names(train_tokens_tfidf_df))


# Time the code execution
start.time <- Sys.time()

# train the model
rpart_cv2 <- train(label ~ ., data = train_tokens_tfidf_df, method = "rpart", 
                    trControl = cv_cntrl, tuneLength = 7)


# Total time of execution on workstation was 42 minutes
total.time <- Sys.time() - start.time
total.time

# Check out our results.
rpart_cv2

# .6012 accuracy is not too bad for a second attempt

#########################################
# Let's do some SVD
# Time the code execution
start.time <- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
train_irlba <- irlba(t(train_tokens_tfidf), nv = 300, maxit = 600)

# Total time of execution on workstation was 17 minutes
total.time <- Sys.time() - start.time
total.time


# Create new feature data frame using our document semantic space of 300
# features (i.e., the V matrix from our SVD).

train_svd = data.frame(label = train$artist, train_irlba$v)

# start parallel processing
cl = makePSOCKcluster(7)
registerDoParallel(cl)

# Time the code execution
start.time <- Sys.time()

# This will be the last run using single decision trees. With a much smaller
# feature matrix we can now use more powerful methods like the mighty Random
# Forest from now on!
rpart_cv4 <- train(label ~ ., data = train_svd, method = "rpart", 
                    trControl = cv_cntrl, tuneLength = 7)

# Total time of execution on workstation was 1 minute
total.time <- Sys.time() - start.time
total.time

## When you are done:
stopCluster(cl)

# Check out our results.
rpart_cv4

# .6381 accuracy. Getting better.

#########################################
# Let's train a random forest

# start parallel processing
cl = makePSOCKcluster(7)
registerDoParallel(cl)

# Time the code execution
start.time <- Sys.time()

# We have reduced the dimensionality of our data using SVD. Also, the 
# application of SVD allows us to use LSA to simultaneously increase the
# information density of each feature. To prove this out, leverage a 
# mighty Random Forest with the default of 500 trees. We'll also ask
# caret to try 7 different values of mtry to find the mtry value that 
# gives the best result!

rf_cv1 <- train(label ~ ., data = train_svd, method = "rf", 
                 trControl = cv_cntrl, tuneLength = 7)

## When you are done:
stopCluster(cl)
# Total time of execution on workstation was 4.4 hours
total.time <- Sys.time() - start.time
total.time

# Check out our results. .7672 Getting there.
rf_cv1
saveRDS(rf_cv1, "./interim/random_forest_model1.RDS")

# Let's drill-down on the results.
train_svd$label = as.factor(train_svd$label)
confusionMatrix(train_svd$label, rf_cv1$finalModel$predicted)

###################
# Let's engineer some features. We can add sentiment and TTR as features

# add TTR
train_ttr = textstat_lexdiv(train_tokens, measure = "TTR")
train_svd = cbind(train_svd, train_ttr$TTR)

# get only pos and neg values from dict
data_dictionary_LSD2015_pos_neg = data_dictionary_LSD2015[1:2]

# add sentiments
train_lsd = tokens_lookup(train_tokens, dictionary = data_dictionary_LSD2015_pos_neg)
train_dfm_lsd = dfm(train_lsd)
train_sentiment = convert(train_dfm_lsd, to = "data.frame")

train_svd = cbind(train_svd, train_sentiment[,2:3])
train_svd$TTR = train_svd$`train_ttr$TTR`
train_svd$`train_ttr$TTR` = NULL

# since we have missing values in the TTR, we'll have to impute them
# 80% of TTR values are equal to 1, so we'll replace the 496 missing values with 1
train_svd$TTR[is.na(train_svd$TTR)] = 1  # Replace NA in one column
summary(train_svd$TTR)


## Run another random forest on the new data

# start parallel processing
cl = makePSOCKcluster(7)
registerDoParallel(cl)

# Time the code execution
start.time <- Sys.time()

# We have reduced the dimensionality of our data using SVD. Also, the 
# application of SVD allows us to use LSA to simultaneously increase the
# information density of each feature. To prove this out, leverage a 
# mighty Random Forest with the default of 500 trees. We'll also ask
# caret to try 7 different values of mtry to find the mtry value that 
# gives the best result!

rf_cv2 <- train(label ~ ., data = train_svd, method = "rf", 
                 trControl = cv_cntrl, tuneLength = 7)

## When you are done:
stopCluster(cl)
# Total time of execution on workstation was 4.8 hours
total.time <- Sys.time() - start.time
total.time

# Check out our results. .7613. So the new features made the performance worse.
rf_cv2
saveRDS(rf_cv2, "./interim/random_forest_model2.RDS")

# Let's drill-down on the results.
train_svd$label = as.factor(train_svd$label)
confusionMatrix(train_svd$label, rf_cv2$finalModel$predicted)

# Let's look at feature importance
varImpPlot(rf_cv2$finalModel)

# TTR actually was a good predictor. Let's keep that and delete sentiment
train_svd = train_svd[,-(302:303)]

#################################
# Let's try an xgboost mode with the same data

# Leverage a grid search of hyperparameters for xgboost. See 
# the following presentation for more information:
# https://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1
tune_grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
                         nrounds = c(50, 75, 100),
                         max_depth = 6:8,
                         min_child_weight = c(2.0, 2.25, 2.5),
                         colsample_bytree = c(0.3, 0.4, 0.5),
                         gamma = 0,
                         subsample = 1)
View(tune_grid)

# start parallel processing
cl = makePSOCKcluster(7)
registerDoParallel(cl)

# Train the xgboost model using 10-fold CV repeated 3 times 
# and a hyperparameter grid search to train the optimal model.

xgb_cv1 <- train(label ~ ., 
                  data = train_svd,
                  method = "xgbTree",
                  tuneGrid = tune_grid,
                  trControl = cv_cntrl)
## When you are done:
stopCluster(cl)

saveRDS(xgb_cv1, "./interim/xgb_model1.RDS")

# Examine caret's processing results
xgb_cv1

results_df = xgb_cv1$results

results_df %>% 
  arrange(desc(Accuracy)) %>% 
  top_n(10)
```

```{r ml-classification-2, eval=FALSE}
# Accuracy dropped to .7621. Random Forest performed better.
# However, let's see if we can change the data so we get above .8
# First, we delete duplicate rows
# Second, we keep stop words
# Third, we don't stem

# get only text and label (=artist)
lyrics = readRDS("./interim/lyrics_tidy.RDS")

ml_lyrics = lyrics %>% 
  select(line, artist) %>% 
  distinct()

# Use caret to create a 70%/30% stratified split. Set the random
# seed for reproducibility
set.seed(42)
indexes = createDataPartition(ml_lyrics$artist, times = 1,
                               p = 0.7, list = FALSE)

train = ml_lyrics[indexes,]
test = ml_lyrics[-indexes,]

# Verify proportions
prop.table(table(train$artist))
prop.table(table(test$artist))

# preprocess training text data
# tokenize
train_tokens = tokens(train$line, what = "word1", 
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Lowercase the tokens
train_tokens = tokens_tolower(train_tokens)

# remove two letter tokens
train_tokens = tokens_select(train_tokens, min_nchar = 3)

# add bigrams and trigrams
train_tokens = tokens_ngrams(train_tokens, n = 1:3)

# Create bag-of-words model (document frequency matrix/ dfm)
train_dfm = dfm(train_tokens, tolower = FALSE)

# Transform to a matrix and inspect.
train_tokens_matrix = as.matrix(train_dfm)

#View(train_tokens_matrix[1:20, 1:100])
dim(train_tokens_matrix)

# Setup a the feature data frame with labels.
train_tokens_df = cbind(label = train$artist, data.frame(train_dfm))

# Cleanup column names.
names(train_tokens_df) = make.names(names(train_tokens_df))

# Let's turn the dfm into a tf-idf-matrix
train_tokens_tfidf = dfm_tfidf(train_dfm)

# Make a clean data frame.
train_tokens_tfidf_df <- cbind(label = train$artist, data.frame(train_tokens_tfidf))
names(train_tokens_tfidf_df) <- make.names(names(train_tokens_tfidf_df))

# Let's do some SVD
# Time the code execution
start.time <- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
train_irlba <- irlba(t(train_tokens_tfidf), nv = 300, maxit = 600)

# Total time of execution on workstation was 12 minutes
total.time <- Sys.time() - start.time
total.time


# Create new feature data frame using our document semantic space of 300
# features (i.e., the V matrix from our SVD).

train_svd = data.frame(label = train$artist, train_irlba$v)

## Run another random forest on the new data

# Use caret to create stratified folds for 10-fold cross validation repeated 
# 3 times (i.e., create 30 random stratified samples)
set.seed(42)
cv_folds = createMultiFolds(train$artist, k = 10, times = 3)

cv_cntrl <- trainControl(method = "repeatedcv", number = 10,
                         repeats = 3, index = cv_folds)



# start parallel processing
cl = makePSOCKcluster(7)
registerDoParallel(cl)

# Time the code execution
start.time <- Sys.time()

rf_cv3 <- train(label ~ ., data = train_svd, method = "rf", 
                 trControl = cv_cntrl, tuneLength = 7)

## When you are done:
stopCluster(cl)
# Total time of execution on workstation was 1.7 hours
total.time <- Sys.time() - start.time
total.time

# Check out our results. .7613. So the new features made the performance worse.
rf_cv3
saveRDS(rf_cv3, "./interim/random_forest_model3.RDS")

# Let's drill-down on the results.
train_svd$label = as.factor(train_svd$label)
confusionMatrix(train_svd$label, rf_cv3$finalModel$predicted)

# Let's look at feature importance
varImpPlot(rf_cv3$finalModel)

```

```{r ml-classification-3, eval=FALSE}
# Accuracy dropped to .7337. Let's try shingles

# get only text and label (=artist)
lyrics = readRDS("./interim/lyrics_tidy.RDS")

ml_lyrics = lyrics %>% 
  select(line, artist)

# Use caret to create a 70%/30% stratified split. Set the random
# seed for reproducibility
set.seed(42)
indexes = createDataPartition(ml_lyrics$artist, times = 1,
                               p = 0.7, list = FALSE)

train = ml_lyrics[indexes,]
test = ml_lyrics[-indexes,]

# Verify proportions
prop.table(table(train$artist))
prop.table(table(test$artist))

train_dfm <- train$line %>%
  tokens(what = "character") %>%
  tokens_keep("[A-Za-z]", valuetype = "regex") %>%
  tokens_ngrams(n = 2, concatenator = "") %>%
  dfm()

# Transform to a matrix and inspect.
train_tokens_matrix = as.matrix(train_dfm)

#View(train_tokens_matrix[1:20, 1:100])
dim(train_tokens_matrix)

# Setup a the feature data frame with labels.
train_tokens_df = cbind(label = train$artist, data.frame(train_dfm))

# Cleanup column names.
names(train_tokens_df) = make.names(names(train_tokens_df))

# Let's turn the dfm into a tf-idf-matrix
train_tokens_tfidf = dfm_tfidf(train_dfm)

# Make a clean data frame.
train_tokens_tfidf_df <- cbind(label = train$artist, data.frame(train_tokens_tfidf))
names(train_tokens_tfidf_df) <- make.names(names(train_tokens_tfidf_df))

# Let's do some SVD
# Time the code execution
start.time <- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
train_irlba <- irlba(t(train_tokens_tfidf), nv = 300, maxit = 600)

# Total time of execution on workstation was 1 minute
total.time <- Sys.time() - start.time
total.time


# Create new feature data frame using our document semantic space of 300
# features (i.e., the V matrix from our SVD).

train_svd = data.frame(label = train$artist, train_irlba$v)

## Run another random forest on the new data

# Use caret to create stratified folds for 10-fold cross validation repeated 
# 3 times (i.e., create 30 random stratified samples)
set.seed(42)
cv_folds = createMultiFolds(train$artist, k = 10, times = 3)

cv_cntrl <- trainControl(method = "repeatedcv", number = 10,
                         repeats = 3, index = cv_folds)

# start parallel processing
cl = makePSOCKcluster(7)
registerDoParallel(cl)

# Time the code execution
start.time <- Sys.time()

rf_cv4 <- train(label ~ ., data = train_svd, method = "rf", 
                 trControl = cv_cntrl, tuneLength = 7)

## When you are done:
stopCluster(cl)
# Total time of execution on workstation was 4.2 hours
total.time <- Sys.time() - start.time
total.time

# Check out our results. .7945. What an improvement! Shingles all the way.
rf_cv4
saveRDS(rf_cv4, "./interim/random_forest_model4.RDS")

# Let's drill-down on the results.
train_svd$label = as.factor(train_svd$label)
confusionMatrix(train_svd$label, rf_cv4$finalModel$predicted)

# Let's look at feature importance
varImpPlot(rf_cv4$finalModel)

```

```{r ml-classification-5, eval=FALSE}
### Now let's try to padding so thta words that do not appear next to each other
### stay away from each other
# No stemming
# No stopword removal

# get only text and label (=artist)
lyrics = readRDS("./interim/lyrics_tidy.RDS")

ml_lyrics = lyrics %>% 
  select(line, artist)

# Use caret to create a 70%/30% stratified split. Set the random
# seed for reproducibility
set.seed(42)
indexes = createDataPartition(ml_lyrics$artist, times = 1,
                               p = 0.7, list = FALSE)

train = ml_lyrics[indexes,]
test = ml_lyrics[-indexes,]

# Verify proportions
prop.table(table(train$artist))
prop.table(table(test$artist))

# preprocess training text data
# tokenize
train_tokens = tokens(train$line, what = "word1", padding = TRUE,
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Lowercase the tokens
train_tokens = tokens_tolower(train_tokens)

# add bigrams and trigrams
train_tokens = tokens_ngrams(train_tokens, n = 1:3)

# Create bag-of-words model (document frequency matrix/ dfm)
train_dfm = dfm(train_tokens, tolower = FALSE)

# Transform to a matrix and inspect.
train_tokens_matrix = as.matrix(train_dfm)

#View(train_tokens_matrix[1:20, 1:100])
dim(train_tokens_matrix)

# Setup a the feature data frame with labels.
train_tokens_df = cbind(label = train$artist, data.frame(train_dfm))

# Cleanup column names.
names(train_tokens_df) = make.names(names(train_tokens_df))

# Let's turn the dfm into a tf-idf-matrix
train_tokens_tfidf = dfm_tfidf(train_dfm)

# Make a clean data frame.
train_tokens_tfidf_df <- cbind(label = train$artist, data.frame(train_tokens_tfidf))
names(train_tokens_tfidf_df) <- make.names(names(train_tokens_tfidf_df))

# Let's do some SVD
# Time the code execution
start.time <- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
train_irlba <- irlba(t(train_tokens_tfidf), nv = 300, maxit = 600)

# Total time of execution on workstation was 11 minutes
total.time <- Sys.time() - start.time
total.time


# Create new feature data frame using our document semantic space of 300
# features (i.e., the V matrix from our SVD).

train_svd = data.frame(label = train$artist, train_irlba$v)

## Run another random forest on the new data

# Use caret to create stratified folds for 10-fold cross validation repeated 
# 3 times (i.e., create 30 random stratified samples)
set.seed(42)
cv_folds = createMultiFolds(train$artist, k = 10, times = 3)

cv_cntrl <- trainControl(method = "repeatedcv", number = 10,
                         repeats = 3, index = cv_folds)

# start parallel processing
cl = makePSOCKcluster(7)
registerDoParallel(cl)

# Time the code execution
start.time <- Sys.time()

rf_cv5 <- train(label ~ ., data = train_svd, method = "rf", 
                 trControl = cv_cntrl, tuneLength = 7)

## When you are done:
stopCluster(cl)
# Total time of execution was 2.9 hours  
total.time <- Sys.time() - start.time
total.time

# Check out our results. .8259 wow, padded tf-idf into SVD!
rf_cv5
saveRDS(rf_cv5, "./interim/random_forest_model5.RDS")

# Let's drill-down on the results.
train_svd$label = as.factor(train_svd$label)
confusionMatrix(train_svd$label, rf_cv5$finalModel$predicted)

# Let's look at feature importance
varImpPlot(rf_cv5$finalModel)

```



```{r ml-classification-test, eval=FALSE}
# get only text and label (=artist)
lyrics = readRDS("./interim/lyrics_tidy.RDS")

ml_lyrics = lyrics %>% 
  select(line, artist)

# Use caret to create a 70%/30% stratified split. Set the random
# seed for reproducibility
set.seed(42)
indexes = createDataPartition(ml_lyrics$artist, times = 1,
                               p = 0.7, list = FALSE)

train = ml_lyrics[indexes,]
test = ml_lyrics[-indexes,]

# Verify proportions
prop.table(table(train$artist))
prop.table(table(test$artist))

# preprocess testing text data
# tokenize
test_tokens = tokens(test$line, what = "word1", padding = TRUE,
                               remove_numbers = TRUE, remove_punct = TRUE,
                               remove_symbols = TRUE, remove_hyphens = TRUE,
                               remove_twitter = TRUE)

# Lowercase the tokens
test_tokens = tokens_tolower(test_tokens)

# add bigrams and trigrams
test_tokens = tokens_ngrams(test_tokens, n = 1:3)

test_tokens <- tokens_select(test_tokens, pattern = train_dfm@Dimnames$features,
                              selection = "keep")

# Create bag-of-words model (document frequency matrix/ dfm)
test_dfm = dfm(test_tokens, tolower = FALSE)

# Transform to a matrix and inspect.
test_tokens_matrix = as.matrix(test_dfm)

#View(test_tokens_matrix[1:20, 1:100])
dim(test_tokens_matrix)

# Setup a the feature data frame with labels.
test_tokens_df = cbind(label = test$artist, data.frame(test_dfm))

# Cleanup column names.
names(test_tokens_df) = make.names(names(test_tokens_df))

# Let's turn the dfm into a tf-idf-matrix
test_tokens_tfidf = dfm_tfidf(test_dfm)

# Make a clean data frame.
test_tokens_tfidf_df <- cbind(label = test$artist, data.frame(test_tokens_tfidf))
names(test_tokens_tfidf_df) <- make.names(names(test_tokens_tfidf_df))

# Let's do some SVD
# Time the code execution
start.time <- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
test_irlba <- irlba(t(test_tokens_tfidf), nv = 300, maxit = 600)

# Total time of execution on workstation was 11 minutes
total.time <- Sys.time() - start.time
total.time

# Lastly, we can now build the test data frame to feed into our trained
# machine learning model for predictions. First up, add Label and TextLength.
test_svd <- data.frame(label = test$artist, test_svd_raw)


# Now we can make predictions on the test data set using our trained mighty 
# random forest.
preds = predict(rf_cv5, test_svd)

# Drill-in on results
test_svd$label = as.factor(test_svd$label)
confusionMatrix(preds, test_svd$label)

### This is very bad. I don't know (yet) what's going on here and why only SY is predicted all the time.
# It's either a super overfitted model or the projection in the tf idf and svd space doesn't work.

# Anyway, let's move on to some Tensor Flow

```


```{r ml-classification-6, eval=FALSE}
# Let's try some tensor flow models

# get only text and label (=artist)
lyrics = readRDS("./interim/lyrics_tidy.RDS")

ml_lyrics = lyrics %>% 
  select(line, artist, song) %>% 
  group_by(artist, song) %>% 
  # paste all the lines per album
  summarize(lyrics = paste0(line, collapse = " ")) %>% 
  select(artist, lyrics)

# Use caret to create a 70%/30% stratified split. Set the random
# seed for reproducibility
set.seed(42)
indexes = createDataPartition(ml_lyrics$artist, times = 1,
                               p = 0.7, list = FALSE)

train = ml_lyrics[indexes,]
test = ml_lyrics[-indexes,]

# Verify proportions
prop.table(table(train$artist))
prop.table(table(test$artist))

# https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/

num_words = 10000
max_length = 50
text_vectorization = layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length,
)

text_vectorization %>% 
  adapt(ml_lyrics$lyrics)

# TODO see https://github.com/tensorflow/tensorflow/pull/34529
get_vocabulary(text_vectorization)

text_vectorization(matrix(ml_lyrics$lyrics[1], ncol = 1))

# build the model
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(input, output)

# Now, configure the model to use an optimizer and a loss function:
model %>% compile(
  optimizer = 'rmsprop',
  loss = 'binary_crossentropy',
  metrics = list('accuracy')
)

history = model %>% fit(
  train$lyrics,
  as.numeric(train$artist == "Sonic Youth"),
  epochs = 500,
  batch_size = 512,
  validation_split = 0.2,
  verbose=2
)

results <- model %>% evaluate(test$lyrics, as.numeric(test$artist == "Sonic Youth"), verbose = 0)
results
# .8002 accuracy on the test set. This is decent and will do!
plot(history)

# save model
model %>% save_model_tf("model")

```

```{r build-lyrics-classifier, dim=c(5,3)}
# load model
model <- load_model_tf("model")
# define input
input = "goo"

# make prediction
pred = model %>% predict(input)
predictions = data.frame(prediction = "prediction",
                         pred = c(0, pred-0.5))

color = ifelse(predictions$pred > 0, sy_colors[3], sw_colors[3])

ggplot(predictions, aes(x=prediction,y=pred[2]))+
  geom_point(size = abs(predictions$pred)*80, color = color)+
  ylim(-1,1)+
    geom_vline(xintercept = 1, color = "grey", alpha = .2)+
    geom_hline(yintercept = 0, color = "grey", alpha = .2)+

    coord_flip()+
  annotate("text", x = 1, y = -0.5, label = "SWANS",
           colour=sw_colors[2], size=7, family="Courier", fontface="bold")+
  annotate("text", x = 1, y = 0.5, label = "SONIC YOUTH",
           colour=sy_colors[2], size=7, family="Courier", fontface="bold")+
  theme_void()

```

